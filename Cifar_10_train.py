# cifar_10_train.py
#
# è¿™æ˜¯ä¸€ä¸ªç”¨äº Cifar-10 åˆ†ç±» (g) å’Œå¤„ç†æ•°æ®ä¸å¹³è¡¡ (h) çš„ç‹¬ç«‹è„šæœ¬ã€‚
#
# å®Œæ•´åŠŸèƒ½åŒ…æ‹¬:
# 1. (g) è®­ç»ƒä¸€ä¸ª 10 åˆ†ç±»æ¨¡å‹ã€‚
# 2. (g) è®­ç»ƒåï¼Œå¯è§†åŒ– 8 ä¸ªæ ·æœ¬çš„é¢„æµ‹ç»“æœ (æ­£ç¡®/é”™è¯¯)ã€‚
# 3. (h) æ¼”ç¤ºä¸¤ç§å¤„ç†æ•°æ®ä¸å¹³è¡¡çš„æ–¹æ³•ã€‚

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, Subset, WeightedRandomSampler
import torchvision
from torchvision import datasets, models, transforms
from torchvision.models import ResNet18_Weights  # ä½¿ç”¨æ–°çš„ API
import numpy as np
import pandas as pd
import time
import copy
import os
import random
import matplotlib.pyplot as plt  # å¯¼å…¥å¯è§†åŒ–åº“

print(f"Torch Version: {torch.__version__}")
print(f"Torchvision Version: {torchvision.__version__}")

# --- (g) å’Œ (h) ä»»åŠ¡çš„é€šç”¨è®¾ç½® ---

# è®¾ç½®è®¾å¤‡
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Cifar-10 ç±»åˆ«
class_names = ('plane', 'car', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck')
num_classes = 10
batch_size = 64
num_epochs = 1  # ä¸ºæ¼”ç¤ºç¼©çŸ­ Epochï¼Œä½ å¯ä»¥å¢åŠ åˆ° 25 æˆ–æ›´å¤š

# (g) Cifar-10 å›¾åƒå°ºå¯¸è°ƒæ•´
# ResNet é¢„è®­ç»ƒè¾“å…¥ä¸º 224x224ï¼Œä½† Cifar-10 ä¸º 32x32
# æˆ‘ä»¬å¿…é¡»å°†å›¾åƒæ”¾å¤§ï¼Œå¹¶ä½¿ç”¨ ImageNet çš„å‡å€¼å’Œæ ‡å‡†å·®
data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(224),  # éšæœºè£å‰ªå¹¶æ”¾å¤§åˆ° 224x224
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'valid': transforms.Compose([
        transforms.Resize(256),  # å…ˆæ”¾å¤§åˆ° 256
        transforms.CenterCrop(224),  # ä¸­å¿ƒè£å‰ªåˆ° 224
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}


# --- ä»ä½ é¡¹ç›®ä¸­å¤åˆ¶çš„è¾…åŠ©å‡½æ•° ---

def set_parameter_requires_grad(model, feature_extracting):
    """
    å¦‚æœ feature_extracting = Trueï¼Œåˆ™å†»ç»“æ‰€æœ‰å±‚
    """
    if feature_extracting:
        for param in model.parameters():
            param.requires_grad = False


def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):
    """
    åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶é‡ç½®æœ€åçš„å…¨è¿æ¥å±‚ã€‚
    (ä»æ‚¨çš„ main.py å¤åˆ¶å¹¶æ›´æ–°äº† API)
    """
    model_ft = None
    input_size = 0

    if model_name == "resnet18":
        # ä½¿ç”¨æ–°çš„ 'weights' API æ¥é¿å…è­¦å‘Š
        weights_param = None
        if use_pretrained:
            weights_param = ResNet18_Weights.DEFAULT

        model_ft = models.resnet18(weights=weights_param)

        set_parameter_requires_grad(model_ft, feature_extract)
        num_ftrs = model_ft.fc.in_features
        model_ft.fc = nn.Linear(num_ftrs, num_classes)
        input_size = 224
    else:
        print("Invalid model name, exiting...")
        exit()

    return model_ft, input_size


def train_model(model, dataloaders, criterion, optimizer, num_epochs=5):
    """
    è®­ç»ƒå¾ªç¯ã€‚
    (ä»æ‚¨çš„ main.py å¤åˆ¶è€Œæ¥ï¼Œå¹¶æ›´æ–°ä¸ºè®°å½•æ‰€æœ‰å†å²)
    """
    since = time.time()

    # åˆå§‹åŒ–æ‰€æœ‰å†å²è®°å½•åˆ—è¡¨
    val_acc_history = []
    train_acc_history = []
    train_losses = []
    valid_losses = []

    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    for epoch in range(num_epochs):
        print(f'Epoch {epoch}/{num_epochs - 1}')
        print('-' * 10)

        for phase in ['train', 'valid']:
            if phase == 'train':
                model.train()
            else:
                model.eval()

            running_loss = 0.0
            running_corrects = 0

            # è¿­ä»£æ•°æ®
            for inputs, labels in dataloaders[phase]:
                inputs = inputs.to(device)
                labels = labels.to(device)

                optimizer.zero_grad()

                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)
                    _, preds = torch.max(outputs, 1)

                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / len(dataloaders[phase].dataset)
            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)

            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')

            # è®°å½•æ‰€æœ‰å››ä¸ªæŒ‡æ ‡
            # æ·±åº¦å¤åˆ¶æ¨¡å‹
            if phase == 'valid' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())

            # ä½¿ç”¨ .cpu().item() å°† Tensor è½¬æ¢ä¸º floatï¼Œä»¥ä¾¿ç»˜å›¾
            if phase == 'valid':
                val_acc_history.append(epoch_acc.cpu().item())
                valid_losses.append(epoch_loss)
            if phase == 'train':
                train_acc_history.append(epoch_acc.cpu().item())
                train_losses.append(epoch_loss)

        print()

    time_elapsed = time.time() - since
    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')
    print(f'Best val Acc: {best_acc:4f}')

    model.load_state_dict(best_model_wts)

    # è¿”å›æ‰€æœ‰å†å²è®°å½•
    return model, train_losses, valid_losses, train_acc_history, val_acc_history


# --- (æ–°å¢) ç”¨äºå¯è§†åŒ–çš„è¾…åŠ©å‡½æ•° ---

def im_convert(tensor):
    """
    å°† Tensor å›¾åƒåæ ‡å‡†åŒ–å¹¶è½¬æ¢ä¸ºå¯æ˜¾ç¤ºçš„ numpy æ•°ç»„
    (ä»æ‚¨çš„ main.py å¤åˆ¶è€Œæ¥)
    """
    image = tensor.to("cpu").clone().detach()  # 1. å¤åˆ¶ Tensor åˆ° CPU
    image = image.numpy().squeeze()  # 2. è½¬æ¢ä¸º NumPy æ•°ç»„
    image = image.transpose(1, 2, 0)  # 3. è½¬æ¢ç»´åº¦ (C,H,W) -> (H,W,C)

    # 4. åæ ‡å‡†åŒ– (ä½¿ç”¨ ImageNet å‡å€¼å’Œæ ‡å‡†å·®)
    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))

    image = image.clip(0, 1)  # 5. è£å‰ªåˆ° [0, 1] èŒƒå›´
    return image


# --- (h) ç”¨äºæ¨¡æ‹Ÿä¸å¹³è¡¡çš„è¾…åŠ©å‡½æ•° ---

def create_unbalanced_dataset(full_dataset, minority_classes, reduction_factor=10):
    """
    ä»ä¸€ä¸ªå®Œæ•´çš„æ•°æ®é›†åˆ›å»ºä¸€ä¸ªä¸å¹³è¡¡çš„æ•°æ®é›†ï¼ˆSubsetï¼‰ã€‚
    """
    print(f"\nCreating unbalanced dataset...")
    print(f"Minority classes: {[class_names[i] for i in minority_classes]} (Reduction Factor: {reduction_factor}x)")

    # 1. è·å–æ‰€æœ‰ç›®æ ‡çš„åˆ—è¡¨
    try:
        targets = full_dataset.targets
    except AttributeError:
        targets = [label for _, label in full_dataset]

    indices_to_keep = []
    class_counts = [0] * num_classes

    # 2. è¿­ä»£æ‰€æœ‰æ ·æœ¬
    for i in range(len(full_dataset)):
        label = targets[i]
        if label in minority_classes:
            # è¿™æ˜¯å°‘æ•°ç±»ï¼ŒæŒ‰æ¦‚ç‡ä¿ç•™
            if random.random() < (1.0 / reduction_factor):
                indices_to_keep.append(i)
                class_counts[label] += 1
        else:
            # è¿™æ˜¯å¤šæ•°ç±»ï¼Œå§‹ç»ˆä¿ç•™
            indices_to_keep.append(i)
            class_counts[label] += 1

    print("Unbalanced Class Counts:")
    for i in range(num_classes):
        print(f"  {class_names[i]:<10}: {class_counts[i]} samples")

    # 3. åˆ›å»ºä¸€ä¸ª Subset
    unbalanced_subset = Subset(full_dataset, indices_to_keep)
    return unbalanced_subset, class_counts


# --- ä¸»æ‰§è¡ŒåŒº ---

def main():
    # =========================================================================
    # (g) è§£å†³ Cifar-10 (å¹³è¡¡)
    # =========================================================================
    print("\n" + "=" * 30)
    print("ğŸš€ (g) Task: Training on Cifar-10 (Balanced)")
    print("=" * 30)

    # 1. åŠ è½½å¹³è¡¡çš„ Cifar-10 æ•°æ®é›†
    data_dir = './data'
    full_train_dataset = datasets.CIFAR10(root=data_dir, train=True,
                                          download=True, transform=data_transforms['train'])
    test_dataset = datasets.CIFAR10(root=data_dir, train=False,
                                    download=True, transform=data_transforms['valid'])

    # 2. åˆ›å»º Dataloaders (å¹³è¡¡)
    dataloaders_balanced = {
        'train': DataLoader(full_train_dataset, batch_size=batch_size, shuffle=True),
        'valid': DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    }

    # 3. åˆå§‹åŒ–æ¨¡å‹
    # æˆ‘ä»¬ä½¿ç”¨è¿ç§»å­¦ä¹  (feature_extract=True)
    model_g, _ = initialize_model("resnet18", num_classes, feature_extract=True, use_pretrained=True)
    model_g = model_g.to(device)

    # 4. è®¾ç½®ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•° (æ ‡å‡†)
    # åªä¼˜åŒ–æ–°æ·»åŠ çš„ fc å±‚çš„å‚æ•°
    params_to_update_g = [param for param in model_g.parameters() if param.requires_grad]
    optimizer_g = optim.Adam(params_to_update_g, lr=0.001)

    criterion_g = nn.CrossEntropyLoss()  # æ ‡å‡†æŸå¤±

    # 5. è®­ç»ƒ
    # 'valid' é›†å°±æ˜¯ Cifar-10 çš„æµ‹è¯•é›†ï¼Œæ‰€ä»¥ "Best val Acc" å°±æ˜¯æˆ‘ä»¬çš„æµ‹è¯•é›†ç»“æœ
    print("Training model for (g)...")
    model_g, train_losses_g, valid_losses_g, train_acc_g, valid_acc_g = train_model(
        model_g, dataloaders_balanced, criterion_g, optimizer_g, num_epochs=num_epochs
    )

    print("\nâœ… (g) Task Complete. 'Best val Acc' is the result on the Cifar-10 testing set.")

    # =========================================================================
    # (g) æ–°å¢ï¼š(ä¸Šä¸€æ­¥å·²æ·»åŠ ) ç»˜åˆ¶è®­ç»ƒå†å²æŠ˜çº¿å›¾
    # =========================================================================
    print("\n" + "=" * 30)
    print("ğŸ“Š æ­£åœ¨ä¸º (g) ä»»åŠ¡ç”Ÿæˆè®­ç»ƒå†å²æŠ˜çº¿å›¾...")
    print("=" * 30)

    try:
        num_epochs_ran_g = len(train_losses_g)
        if num_epochs_ran_g > 0:
            epochs_range = range(1, num_epochs_ran_g + 1)

            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
            fig.suptitle('Task (g) Training History (Cifar-10)')

            # ç»˜åˆ¶ æŸå¤± (Loss)
            ax1.plot(epochs_range, train_losses_g, 'b-o', label='Training Loss')
            ax1.plot(epochs_range, valid_losses_g, 'r-o', label='Validation Loss')
            ax1.set_ylabel('Loss')
            ax1.set_title('Training and Validation Loss')
            ax1.legend()
            ax1.grid(True)

            # ç»˜åˆ¶ å‡†ç¡®ç‡ (Accuracy)
            ax2.plot(epochs_range, train_acc_g, 'b-o', label='Training Accuracy')
            ax2.plot(epochs_range, valid_acc_g, 'r-o', label='Validation Accuracy')
            ax2.set_xlabel('Epoch')
            ax2.set_ylabel('Accuracy')
            ax2.set_title('Training and Validation Accuracy')
            ax2.legend()
            ax2.grid(True)

            plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # è°ƒæ•´å¸ƒå±€
            print("...è¯·æŸ¥çœ‹å¼¹å‡ºçš„å›¾è¡¨çª—å£ã€‚")
            plt.show()
            print("âœ… (g) ä»»åŠ¡æŠ˜çº¿å›¾æ˜¾ç¤ºå®Œæ¯•ã€‚")
        else:
            print("âš ï¸ è­¦å‘Š: (g) ä»»åŠ¡æ²¡æœ‰ epoch æ•°æ®å¯ä¾›ç»˜å›¾ã€‚")

    except Exception as e:
        print(f"âš ï¸ è­¦å‘Š: ç»˜åˆ¶ (g) ä»»åŠ¡å›¾è¡¨æ—¶å‡ºé”™: {e}")

    # =========================================================================
    # (g) æ–°å¢ï¼šå¯è§†åŒ–åˆ†ç±»ç»“æœ
    # =========================================================================
    print("\n" + "=" * 30)
    print("ğŸ“¸ æ­£åœ¨å¯è§†åŒ– (g) ä»»åŠ¡çš„åˆ†ç±»ç»“æœ...")
    print("=" * 30)
    print("ç»¿è‰² = æ­£ç¡®, çº¢è‰² = é”™è¯¯")
    print("æ ¼å¼: é¢„æµ‹ç»“æœ (çœŸå®æ ‡ç­¾)")
    print("...è¯·æŸ¥çœ‹å¼¹å‡ºçš„çª—å£ã€‚å…³é—­çª—å£åç¨‹åºå°†ç»§ç»­æ‰§è¡Œ (h) ä»»åŠ¡...")

    # 1. ä»æµ‹è¯•é›†ï¼ˆéªŒè¯é›†ï¼‰è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®
    dataiter = iter(dataloaders_balanced['valid'])
    images, labels = next(dataiter)

    # 2. å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ (æˆ‘ä»¬ä½¿ç”¨ (g) ä»»åŠ¡è®­ç»ƒå¥½çš„ model_g)
    model_g.eval()

    # 3. å°†å›¾åƒä¼ å…¥æ¨¡å‹è·å–é¢„æµ‹ç»“æœ
    if device.type == 'cuda':
        output = model_g(images.cuda())
    else:
        output = model_g(images)

    # 4. è·å–é¢„æµ‹çš„ç±»åˆ«ç´¢å¼•
    _, preds_tensor = torch.max(output, 1)
    preds = np.squeeze(preds_tensor.cpu().numpy())

    # 5. ç»˜åˆ¶å›¾åƒå’Œç»“æœ (å¤åˆ¶è‡ªæ‚¨çš„ main.py)
    fig = plt.figure(figsize=(20, 10))  # 2è¡Œ4åˆ—
    columns = 4
    rows = 2

    for idx in range(columns * rows):
        ax = fig.add_subplot(rows, columns, idx + 1, xticks=[], yticks=[])
        # ä½¿ç”¨æˆ‘ä»¬åˆšæ·»åŠ çš„ im_convert å‡½æ•°
        ax.imshow(im_convert(images[idx]))

        # class_names å·²åœ¨ cifar_10_train.py é¡¶éƒ¨å®šä¹‰
        pred_label = class_names[preds[idx]]
        true_label = class_names[labels[idx].item()]

        # åˆ¤æ–­å¯¹é”™å¹¶è®¾ç½®é¢œè‰²
        is_correct = (pred_label == true_label)

        ax.set_title(
            f"{pred_label} ({true_label})",
            color=("green" if is_correct else "red")
        )

    # 6. æ˜¾ç¤ºå›¾åƒ
    plt.show()  # ç¨‹åºä¼šåœ¨æ­¤æš‚åœï¼Œç›´åˆ°æ‚¨å…³é—­çª—å£

    # =========================================================================
    # (g) æ–°å¢ï¼šå°†æ‰€æœ‰ 10,000 å¼ æµ‹è¯•å›¾ç‰‡çš„ç»“æœä¿å­˜ä¸º CSV
    # =========================================================================
    print("\n" + "=" * 30)
    print("ğŸ’¾ æ­£åœ¨ä¸º (g) ä»»åŠ¡ç”Ÿæˆå®Œæ•´çš„ CSV é¢„æµ‹æ–‡ä»¶...")
    print("=" * 30)
    print("...è¿™å¯èƒ½éœ€è¦ä¸€ç‚¹æ—¶é—´ï¼Œæ­£åœ¨éå†æ‰€æœ‰ 10,000 å¼ æµ‹è¯•å›¾åƒ...")

    # 1. ç¡®ä¿æ¨¡å‹åœ¨è¯„ä¼°æ¨¡å¼
    model_g.eval()

    all_preds = []  # å­˜å‚¨æ‰€æœ‰é¢„æµ‹
    all_true_labels = []  # å­˜å‚¨æ‰€æœ‰çœŸå®æ ‡ç­¾

    # 2. ç¦ç”¨æ¢¯åº¦ï¼Œéå† *æ•´ä¸ª* æµ‹è¯•é›†
    with torch.no_grad():  #

        # dataloaders_balanced['valid'] åŒ…å«æ•´ä¸ª Cifar-10 æµ‹è¯•é›†
        for inputs, labels in dataloaders_balanced['valid']:
            # å°†æ•°æ®ç§»è‡³è®¾å¤‡ (GPU/CPU)
            inputs = inputs.to(device)

            # è¿è¡Œæ¨¡å‹
            outputs = model_g(inputs)

            # è·å–é¢„æµ‹ç»“æœ
            _, preds_tensor = torch.max(outputs, 1)  #

            # 3. å°†ç»“æœä» GPU/Tensor è½¬æ¢å› CPU/numpy å¹¶å­˜å‚¨
            all_preds.extend(preds_tensor.cpu().numpy())
            all_true_labels.extend(labels.cpu().numpy())

    print(f"å·²å¤„ç† {len(all_preds)} å¼ æµ‹è¯•å›¾åƒã€‚")

    # 4. å°†æ•°å­—æ ‡ç­¾ (0-9) è½¬æ¢ä¸ºå¯è¯»çš„ç±»åˆ«åç§°
    # class_names æ˜¯åœ¨è„šæœ¬é¡¶éƒ¨å®šä¹‰çš„ ('plane', 'car', ...)
    pred_names = [class_names[p] for p in all_preds]
    true_names = [class_names[t] for t in all_true_labels]

    # 5. æ£€æŸ¥æ¯ä¸ªé¢„æµ‹æ˜¯å¦æ­£ç¡®
    correct = (np.array(all_preds) == np.array(all_true_labels))

    # 6. ä½¿ç”¨ Pandas åˆ›å»º DataFrame (ç±»ä¼¼ predict.py)
    results_df = pd.DataFrame({
        'ImageIndex': range(len(all_preds)),
        'PredictedLabel': pred_names,
        'TrueLabel': true_names,
        'IsCorrect': correct
    })

    # 7. ä¿å­˜åˆ° CSV æ–‡ä»¶
    submission_filename = "cifar10_test_results.csv"
    results_df.to_csv(submission_filename, index=False)  #

    print(f"\nâœ… å®Œæ•´çš„é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {submission_filename}")
    print("æ–‡ä»¶å¤´éƒ¨å†…å®¹ (å‰5è¡Œ)ï¼š")
    print(results_df.head())

    # --- [æ–°åŠŸèƒ½] (g) ä»»åŠ¡ï¼šç»˜åˆ¶â€œç±»åˆ«å‡†ç¡®ç‡å¯¹æ¯”å›¾â€ ---
    print("\n" + "=" * 30)
    print("ğŸ“Š æ­£åœ¨ä¸º (g) ä»»åŠ¡ç”Ÿæˆâ€œç±»åˆ«å‡†ç¡®ç‡å¯¹æ¯”å›¾â€...")
    print("=" * 30)

    try:
        # 1. åˆå§‹åŒ–æ¯ä¸ªç±»åˆ«çš„æ­£ç¡®æ•°å’Œæ€»æ•°
        # (æˆ‘ä»¬ä½¿ç”¨ all_preds å’Œ all_true_labels, å®ƒä»¬æ˜¯ä¸Šä¸€æ­¥åˆšç”Ÿæˆçš„)
        class_correct = [0] * num_classes
        class_total = [0] * num_classes

        # 2. éå†æ‰€æœ‰æµ‹è¯•ç»“æœ
        for i in range(len(all_true_labels)):
            true_label = all_true_labels[i]
            pred_label = all_preds[i]

            # ç»Ÿè®¡æ€»æ•°
            class_total[true_label] += 1

            # ç»Ÿè®¡æ­£ç¡®æ•°
            if true_label == pred_label:
                class_correct[true_label] += 1

        # 3. è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
        per_class_accuracy = []
        print("Per-Class Accuracy (Task g):")
        for i in range(num_classes):
            if class_total[i] > 0:
                acc = 100 * class_correct[i] / class_total[i]
                per_class_accuracy.append(acc)
                print(f"  - {class_names[i]:<10}: {acc:.2f}% ({class_correct[i]}/{class_total[i]})")
            else:
                per_class_accuracy.append(0)
                print(f"  - {class_names[i]:<10}: N/A (0 samples)")

        # 4. ç»˜åˆ¶æ¡å½¢å›¾
        plt.figure(figsize=(15, 7))
        plt.bar(class_names, per_class_accuracy, color='skyblue')
        plt.xlabel('Class')
        plt.ylabel('Accuracy (%)')
        plt.title('Per-Class Accuracy on Cifar-10 Test Set (Task g)')
        plt.ylim(0, 100)  # å‡†ç¡®ç‡åœ¨ 0% åˆ° 100% ä¹‹é—´
        plt.grid(axis='y', linestyle='--', alpha=0.7)

        # åœ¨æ¡å½¢å›¾ä¸Šæ˜¾ç¤ºç™¾åˆ†æ¯”
        for i, acc in enumerate(per_class_accuracy):
            plt.text(i, acc + 1, f'{acc:.1f}%', ha='center', color='black')

        print("\n...è¯·æŸ¥çœ‹å¼¹å‡ºçš„â€œç±»åˆ«å‡†ç¡®ç‡å¯¹æ¯”å›¾â€çª—å£ã€‚")
        plt.show()
        print("âœ… â€œç±»åˆ«å‡†ç¡®ç‡å¯¹æ¯”å›¾â€æ˜¾ç¤ºå®Œæ¯•ã€‚")

    except Exception as e:
        print(f"âš ï¸ è­¦å‘Š: ç»˜åˆ¶â€œç±»åˆ«å‡†ç¡®ç‡å¯¹æ¯”å›¾â€æ—¶å‡ºé”™: {e}")
    # --- [æ–°åŠŸèƒ½] ç»˜å›¾ä»£ç ç»“æŸ ---

    # =========================================================================
    # (h) è§£å†³æ•°æ®ä¸å¹³è¡¡é—®é¢˜
    # =========================================================================
    print("\n" + "=" * 30)
    print("ğŸš€ (h) Task: Handling Data Imbalance")
    print("=" * 30)

    # 1. æ¨¡æ‹Ÿä¸€ä¸ªä¸å¹³è¡¡çš„æ•°æ®é›†
    # æˆ‘ä»¬è®© 'bird' (idx=2) å’Œ 'ship' (idx=8) æˆä¸ºå°‘æ•°ç±»
    minority_classes = [2, 8]
    unbalanced_train_subset, class_counts = create_unbalanced_dataset(
        full_train_dataset,
        minorities=minority_classes,
        reduction_factor=10
    )

    # --- (h) æ–¹æ³• 1: åŠ æƒæŸå¤± (Weighted Loss) ---
    print("\n" + "-" * 20)
    print("Running (h) Approach 1: Weighted Loss")
    print("Justification: Penalizes errors on minority classes more heavily.")
    print("-" * 20)

    # 1.1 è®¡ç®—ç±»åˆ«æƒé‡
    weights = 1.0 / torch.tensor(class_counts, dtype=torch.float)
    weights = weights.to(device)

    criterion_h1 = nn.CrossEntropyLoss(weight=weights)

    # 1.2 Dataloader (ä½¿ç”¨ä¸å¹³è¡¡æ•°æ®ï¼Œä½†å¸¸è§„é‡‡æ ·)
    dataloader_h1_train = DataLoader(unbalanced_train_subset, batch_size=batch_size, shuffle=True)
    dataloaders_h1 = {'train': dataloader_h1_train, 'valid': dataloaders_balanced['valid']}

    # 1.3 åˆå§‹åŒ–æ–°æ¨¡å‹
    model_h1, _ = initialize_model("resnet18", num_classes, feature_extract=True, use_pretrained=True)
    model_h1 = model_h1.to(device)
    params_h1 = [p for p in model_h1.parameters() if p.requires_grad]
    optimizer_h1 = optim.Adam(params_h1, lr=0.001)

    # 1.4 è®­ç»ƒ
    print("Training model for (h) Approach 1...")
    model_h1, _, _, _, _ = train_model(model_h1, dataloaders_h1, criterion_h1, optimizer_h1, num_epochs=num_epochs)

    # --- (h) æ–¹æ³• 2: åŠ æƒéšæœºé‡‡æ · (Weighted Random Sampler) ---
    print("\n" + "-" * 20)
    print("Running (h) Approach 2: Weighted Random Sampler")
    print("Justification: Balances data at the batch level by oversampling minorities.")
    print("-" * 20)

    # 2.1 ä¸º *æ¯ä¸ªæ ·æœ¬* è®¡ç®—æƒé‡
    subset_targets = [unbalanced_train_subset.dataset.targets[i] for i in unbalanced_train_subset.indices]
    class_weights = 1.0 / torch.tensor(class_counts, dtype=torch.float)
    sample_weights = [class_weights[target] for target in subset_targets]

    # 2.2 åˆ›å»º Sampler
    sampler = WeightedRandomSampler(weights=sample_weights,
                                    num_samples=len(sample_weights),
                                    replacement=True)

    # 2.3 Dataloader (ä½¿ç”¨ Sampler, **shuffle å¿…é¡»ä¸º False**)
    dataloader_h2_train = DataLoader(unbalanced_train_subset,
                                     batch_size=batch_size,
                                     sampler=sampler)  # shuffle=False (sampler å·²å¤„ç†éšæœºæ€§)

    dataloaders_h2 = {'train': dataloader_h2_train, 'valid': dataloaders_balanced['valid']}

    # 2.4 ä½¿ç”¨ *æ ‡å‡†* æŸå¤±å‡½æ•° (å› ä¸ºæ•°æ®å·²ç»è¢«é‡‡æ ·å¹³è¡¡äº†)
    criterion_h2 = nn.CrossEntropyLoss()

    # 2.5 åˆå§‹åŒ–æ–°æ¨¡å‹
    model_h2, _ = initialize_model("resnet18", num_classes, feature_extract=True, use_pretrained=True)
    model_h2 = model_h2.to(device)
    params_h2 = [p for p in model_h2.parameters() if p.requires_grad]
    optimizer_h2 = optim.Adam(params_h2, lr=0.001)

    # 2.6 è®­ç»ƒ
    print("Training model for (h) Approach 2...")
    model_h2, _, _, _, _ = train_model(model_h2, dataloaders_h2, criterion_h2, optimizer_h2, num_epochs=num_epochs)

    print("\nâœ… (h) Task Complete. Both approaches have been demonstrated.")


if __name__ == '__main__':
    main()